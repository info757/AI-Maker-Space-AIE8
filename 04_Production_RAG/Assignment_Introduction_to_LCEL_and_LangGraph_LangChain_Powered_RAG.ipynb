{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eTBHYNP4g1"
      },
      "source": [
        "# Introduction to LCEL and LangGraph: LangChain Powered RAG\n",
        "\n",
        "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
        "\n",
        "We'll be building a RAG system to answer questions about how people use AI, using the \"How People Use AI\" dataset.\n",
        "\n",
        "In the notebook, you'll complete the following Tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "    1. LangChain and LCEL Concepts\n",
        "    2. Understanding States and Nodes\n",
        "    3. Introduction to QDrant Vector Databases\n",
        "    4. Building a Basic Graph\n",
        "\n",
        "Let's get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation Requirements\n",
        "\n",
        "Also, make sure Ollama is installed and running with the required models pulled (see instructions below).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: LangSmith Setup for Tracing and Monitoring\n",
        "\n",
        "LangSmith provides powerful tracing, monitoring, and debugging capabilities for LangChain applications. While not required for this notebook, setting it up will give you valuable insights into your RAG system's performance.\n",
        "\n",
        "### Getting LangSmith Credentials\n",
        "\n",
        "1. **Sign up for LangSmith**: Visit [smith.langchain.com](https://smith.langchain.com) and create a free account\n",
        "2. **Get your API Key**: \n",
        "   - Go to Settings â†’ API Keys\n",
        "   - Create a new API key and copy it\n",
        "3. **Set your environment variables** (choose one method below):\n",
        "\n",
        "**Option A: Set environment variables in your terminal before starting Jupyter:**\n",
        "```bash\n",
        "export LANGCHAIN_TRACING_V2=true\n",
        "export LANGCHAIN_API_KEY=\"your-api-key-here\"\n",
        "export LANGCHAIN_PROJECT=\"RAG-Assignment\"\n",
        "```\n",
        "\n",
        "**Option B: Set them in the notebook (run the cell below):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith tracing disabled for this assignment\n",
            "Project name: Not set\n"
          ]
        }
      ],
      "source": [
        "# Optional: Set up LangSmith tracing\n",
        "# Uncomment and fill in your credentials if you want to use LangSmith\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Uncomment the lines below to enable LangSmith tracing\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-Assignment\"\n",
        "\n",
        "# Verify setup (uncomment to check)\n",
        "# print(\"LangSmith tracing enabled:\", os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\"))\n",
        "# print(\"Project name:\", os.getenv(\"LANGCHAIN_PROJECT\", \"Not set\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What LangSmith Provides\n",
        "\n",
        "Once set up, LangSmith will automatically trace your LangChain operations and provide:\n",
        "\n",
        "- **Execution traces**: See exactly how your RAG pipeline processes each query\n",
        "- **Performance metrics**: Monitor latency, token usage, and costs\n",
        "- **Debugging tools**: Inspect intermediate outputs at each step\n",
        "- **Error tracking**: Identify and debug issues in your chains\n",
        "- **Dataset management**: Collect and organize your queries and responses\n",
        "\n",
        "You can view all traces and analytics in your LangSmith dashboard at [smith.langchain.com](https://smith.langchain.com).\n",
        "\n",
        "> **Note**: LangSmith is completely optional for this assignment. The notebook will work perfectly fine without it, but it's a valuable tool for production applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ayVXHXHRE_t"
      },
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6wTp9C5qbY"
      },
      "source": [
        "## Set Up Ollama\n",
        "\n",
        "We'll be using Ollama to run local LLM models. Make sure you have Ollama installed and running:\n",
        "\n",
        "1. Install Ollama from https://ollama.ai (`curl https://ollama.ai/install.sh | sh`)\n",
        "2. Make sure the output of `ollama -v` reads `0.11.10` or greater.\n",
        "2. Pull the models we'll use:\n",
        "   ```bash\n",
        "   ollama pull gpt-oss:20b # For the chat model\n",
        "   ollama pull embeddinggemma:latest  # For embeddings\n",
        "   ```\n",
        "3. Ensure Ollama is running (it should start automatically after installation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTsujAkpRWpJ"
      },
      "source": [
        "### A Note On Runnables\n",
        "\n",
        "# Understanding LangChain Runnables and LCEL\n",
        "\n",
        "In LangChain, a Runnable is like a LEGO brick in your AI application - it's a standardized component that can be easily connected with other components. The real power of Runnables comes from their ability to be combined in flexible ways using LCEL (LangChain Expression Language).\n",
        "\n",
        "## Key Features of Runnables\n",
        "\n",
        "### 1. Universal Interface\n",
        "Every Runnable in LangChain follows the same pattern:\n",
        "- Takes an input\n",
        "- Performs some operation\n",
        "- Returns an output\n",
        "\n",
        "This consistency means you can treat different components (like models, retrievers, or parsers) in the same way.\n",
        "\n",
        "### 2. Built-in Parallelization\n",
        "Runnables come with methods for handling multiple inputs efficiently:\n",
        "```python\n",
        "# Process inputs in parallel, maintain order\n",
        "results = chain.batch([input1, input2, input3])\n",
        "\n",
        "# Process inputs as they complete\n",
        "for result in chain.batch_as_completed([input1, input2, input3]):\n",
        "    print(result)\n",
        "```\n",
        "\n",
        "### 3. Streaming Support\n",
        "Perfect for responsive applications:\n",
        "```python\n",
        "# Stream outputs as they're generated\n",
        "for chunk in chain.stream({\"query\": \"Tell me a story\"}):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "```\n",
        "\n",
        "### 4. Easy Composition\n",
        "The `|` operator makes building pipelines intuitive:\n",
        "```python\n",
        "# Create a basic RAG chain\n",
        "rag_chain = retriever | prompt | model | output_parser\n",
        "```\n",
        "\n",
        "## Common Types of Runnables\n",
        "\n",
        "- **Language Models**: Like our `ChatOllama` instance (running locally with Ollama)\n",
        "- **Prompt Templates**: Format inputs consistently\n",
        "- **Retrievers**: Get relevant context from a vector store\n",
        "- **Output Parsers**: Structure model outputs\n",
        "- **LangGraph Nodes**: Individual components in our graph\n",
        "\n",
        "Think of Runnables as the building blocks of your LLM application. Just like how you can combine LEGO bricks in countless ways, you can mix and match Runnables to create increasingly sophisticated applications!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaVlJiilDzwM"
      },
      "source": [
        "## LangGraph Based RAG\n",
        "\n",
        "Now that we have a reasonable grasp of LCEL and the idea of Runnables - let's see how we can use LangGraph to build the same system!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I77-NKo1EowG"
      },
      "source": [
        "### Primer: What is LangGraph?\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "#### Why Cycles?\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "#### Why LangGraph?\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!\n",
        "\n",
        "> NOTE: We're going to focus on building a simple DAG for today's assignment as an introduction to LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLCnMXNE_Qc"
      },
      "source": [
        "### Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "> coordinated multi-actor and stateful applications\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "However, in our example here, we're focusing on a simpler `State` object:\n",
        "\n",
        "```python\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "```\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. **We initialize our state object**:\n",
        "   ```python\n",
        "   {\n",
        "       \"question\": \"\",\n",
        "       \"context\": [],\n",
        "       \"response\": \"\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "2. **Our user submits a query to our application.**  \n",
        "   We store the user's question in `state[\"question\"]`. Now we have:\n",
        "   ```python\n",
        "   {\n",
        "       \"question\": \"How tall is the Eiffel Tower?\",\n",
        "       \"context\": [],\n",
        "       \"response\": \"\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "3. **We pass our state object to an Agent node** which is able to read the current state. It will use the value of `state[\"question\"]` as input and might retrieve some context documents related to the question. It then generates a response which it stores in `state[\"response\"]`. For example:\n",
        "   ```python\n",
        "   {\n",
        "       \"question\": \"How tall is the Eiffel Tower?\",\n",
        "       \"context\": [Document(page_content=\"...some data...\")],\n",
        "       \"response\": \"The Eiffel Tower is about 324 meters tall...\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "That's it! The important part is that we have a consistent object (`State`) that's passed around, holding the crucial information as we go from one node to the next. This ensures our application has a single source of truth about what has happened so far and what is happening now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kxczzsfVFNWT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-openai) (0.3.76)\n",
            "Collecting openai<2.0.0,>=1.104.2 (from langchain-openai)\n",
            "  Downloading openai-1.108.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.29)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/williamholt/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.104.2->langchain-openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.104.2->langchain-openai)\n",
            "  Downloading jiter-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /Users/williamholt/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Collecting tqdm>4 (from openai<2.0.0,>=1.104.2->langchain-openai)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/williamholt/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/williamholt/.venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.18)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "Downloading openai-1.108.2-py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.4/948.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (316 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, jiter, distro, openai, langchain-openai\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5/5\u001b[0m [langchain-openai][openai]\n",
            "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 jiter-0.11.0 langchain-openai-0.3.33 openai-1.108.2 tqdm-4.67.1\n",
            "âœ… langchain_openai installed successfully!\n",
            "Installing langgraph...\n",
            "Requirement already satisfied: langgraph in /Users/williamholt/.venv/lib/python3.12/site-packages (0.6.7)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph) (0.3.76)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph) (2.11.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: anyio in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (0.4.29)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/williamholt/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "âœ… langgraph installed successfully!\n",
            "Installing langchain-qdrant...\n",
            "Requirement already satisfied: langchain-qdrant in /Users/williamholt/.venv/lib/python3.12/site-packages (0.2.1)\n",
            "Requirement already satisfied: qdrant-client<2.0.0,>=1.10.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-qdrant) (1.15.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-qdrant) (2.11.9)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-qdrant) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (0.4.29)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/williamholt/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (0.4.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.75.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (2.3.3)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (6.32.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (2.5.0)\n",
            "Requirement already satisfied: anyio in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (4.10.0)\n",
            "Requirement already satisfied: certifi in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant) (1.3.1)\n",
            "âœ… langchain-qdrant installed successfully!\n",
            "Installing qdrant-client...\n",
            "Requirement already satisfied: qdrant-client in /Users/williamholt/.venv/lib/python3.12/site-packages (1.15.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client) (1.75.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client) (2.3.3)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client) (6.32.1)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client) (2.11.9)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/williamholt/.venv/lib/python3.12/site-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /Users/williamholt/.venv/lib/python3.12/site-packages (from grpcio>=1.41.0->qdrant-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "âœ… qdrant-client installed successfully!\n",
            "ï¿½ï¿½ All packages installed! You can now run the imports.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install langchain_openai directly in the notebook environment\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain-openai\"])\n",
        "\n",
        "print(\"âœ… langchain_openai installed successfully!\")\n",
        "\n",
        "# Install the missing packages in Cursor's Python environment\n",
        "packages = ['langgraph', 'langchain-qdrant', 'qdrant-client']\n",
        "\n",
        "for package in packages:\n",
        "    print(f\"Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "    print(f\"âœ… {package} installed successfully!\")\n",
        "\n",
        "print(\"ï¿½ï¿½ All packages installed! You can now run the imports.\")\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: list[Document]\n",
        "  response: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l6xFY0_HoXG"
      },
      "source": [
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL Runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keL9O1drInw1"
      },
      "source": [
        "### Building Nodes\n",
        "\n",
        "We're going to need two nodes:\n",
        "\n",
        "A node for retrieval, and a node for generation.\n",
        "\n",
        "Let's start with our `retrieve` node!\n",
        "\n",
        "Notice how we do not need to update the state object in the node, but can instead return a modification directly to our state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Building a Retriever with LangChain\n",
        "\n",
        "In order to build our `retrieve` node, we'll first need to build a retriever!\n",
        "\n",
        "This will involve the following steps: \n",
        "\n",
        "1. Ingesting Data\n",
        "2. Chunking the Data\n",
        "3. Vectorizing the Data and Storing it in a Vector Database\n",
        "4. Converting it to a Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Retreiver Step 1: Ingesting Data\n",
        "\n",
        "In today's lesson, we're going to be building a RAG system to answer questions about how people use AI - and we will pull information into our index (vectorized chunks stored in our vector store) through LangChain's [`PyMuPDFLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: We'll be using an async loader during our document ingesting - but our Jupyter Kernel is already running in an asyc loop! This means we'll want the ability to *nest* async loops. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we're good to load our documents through the [`PyMuPDFLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing langchain-community...\n",
            "Requirement already satisfied: langchain-community in /Users/williamholt/.venv/lib/python3.12/site-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (0.3.76)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (0.4.29)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-community) (2.3.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/williamholt/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "âœ… langchain-community installed successfully!\n",
            "Installing pymupdf...\n",
            "Requirement already satisfied: pymupdf in /Users/williamholt/.venv/lib/python3.12/site-packages (1.26.4)\n",
            "âœ… pymupdf installed successfully!\n",
            "ï¿½ï¿½ All packages installed! You can now run the imports.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install the missing packages in Cursor's Python environment\n",
        "packages = ['langchain-community', 'pymupdf']\n",
        "\n",
        "for package in packages:\n",
        "    print(f\"Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "    print(f\"âœ… {package} installed successfully!\")\n",
        "\n",
        "print(\"ï¿½ï¿½ All packages installed! You can now run the imports.\")\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "\n",
        "ai_usage_knowledge_resources = directory_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CHATGPT\\nAaron Chatterji\\nThomas Cunningham\\nDavid J. Deming\\nZoe Hitzig\\nChristopher Ong\\nCarl Yan Shan\\nKevin Wadman\\nWorking Paper 34255\\nhttp://www.nber.org/papers/w34255\\nNATIONAL BUREAU OF ECONOMIC RESEARCH\\n1050 Massachusetts Avenue\\nCambridge, MA 02138\\nSeptember 2025\\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan \\nBeiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, \\nHarrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We \\nespecially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for \\nthis work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all \\ncode run to produce the analyses in this paper is available on request. The views expressed herein \\nare those of the authors and do not necessarily reflect the views of the National Bureau of \\nEconomic Research.\\nAt least one co-author has disclosed a'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_usage_knowledge_resources[0].page_content[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TextSplitting aka Chunking\n",
        "\n",
        "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
        "\n",
        "It will split based on the following rules:\n",
        "\n",
        "- Each chunk has a maximum size of 1000 tokens\n",
        "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
        "\n",
        "Let's implement it and see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing tiktoken...\n",
            "Requirement already satisfied: tiktoken in /Users/williamholt/.venv/lib/python3.12/site-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/williamholt/.venv/lib/python3.12/site-packages (from tiktoken) (2025.9.18)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
            "âœ… tiktoken installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install tiktoken in Cursor's Python environment\n",
        "print(\"Installing tiktoken...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tiktoken\"])\n",
        "print(\"âœ… tiktoken installed successfully!\")\n",
        "\n",
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    # Using cl100k_base encoding which is a good general-purpose tokenizer\n",
        "    # This works well for estimating token counts even with Ollama models\n",
        "    tokens = tiktoken.get_encoding(\"cl100k_base\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_usage_knowledge_chunks = text_splitter.split_documents(ai_usage_knowledge_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
        "\n",
        "Brainstorm some ideas that would split large single documents into smaller documents.\n",
        "\n",
        "1. Semantic chunking with overlap to reduce context fragmentation\n",
        "2. Content Aware chunking. Create chunk size based on type of content. \n",
        "3. Topic based chunking. Split chunks by topics/sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings and Dense Vector Search\n",
        "\n",
        "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
        "\n",
        "This sounds like a perfect job for embeddings!\n",
        "\n",
        "We'll be using Ollama's `embeddinggemma` model as our embedding model today! This is a powerful open-source embedding model that runs locally.\n",
        "\n",
        "Let's load it up through LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Using OpenAI's text-embedding-3-small model (1536 dimensions)\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### â“ Question #1:\n",
        "\n",
        "What is the embedding dimension, given that we're using `embeddinggemma`?\n",
        "\n",
        "You will need to fill the next cell out correctly with your embedding dimension for the rest of the notebook to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_dim = 1536 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using A Vector Database - Intoduction to Qdrant\n",
        "\n",
        "Up to this point, we've been using a dictionary to hold our embeddings - typically, we'll want to use a more robust strategy.\n",
        "\n",
        "In this bootcamp - we'll be focusing on leveraging [Qdrant's vector database](https://qdrant.tech/qdrant-vector-database/).\n",
        "\n",
        "Let's take a look at how we set-up Qdrant!\n",
        "\n",
        "> NOTE: We'll be spending a lot of time learning about Qdrant throughout the remainder of our time together - but for an initial primer, please check out [this resource](https://qdrant.tech/articles/what-is-a-vector-database/)\n",
        "\n",
        "We are going to be using an \"in-memory\" Qdrant client, which means that our vectors will be held in our system's memory (RAM) - this is useful for prototyping and developement at smaller scales - but would need to be modified when moving to production. Luckily for us, this modification is trivial!\n",
        "\n",
        "> NOTE: While LangChain uses the terminology \"VectorStore\" (also known as a Vector Library), Qdrant is a \"Vector Database\" - more info. on that [here.](https://weaviate.io/blog/vector-library-vs-vector-database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to create a collection - a collection is a specific...collection of vectors within the Qdrant client.\n",
        "\n",
        "These are useful as they allow us to create multiple different \"warehouses\" in a single client, which can be leveraged for personalization and more!\n",
        "\n",
        "Also notice that we define what our vector shapes are (embedding dim) as well as our desired distance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.create_collection(\n",
        "    collection_name=\"ai_usage_knowledge_index\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can assemble our vector database! Notice that we provide our client, our created collection, and our embedding model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Collection deleted successfully\n",
            "âœ… Collection created successfully\n",
            "âœ… Vector store created successfully\n"
          ]
        }
      ],
      "source": [
        "# Delete the existing collection\n",
        "try:\n",
        "    client.delete_collection(\"ai_usage_knowledge_index\")\n",
        "    print(\"âœ… Collection deleted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Collection deletion error: {e}\")\n",
        "\n",
        "# Create the collection with the correct dimensions\n",
        "client.create_collection(\n",
        "    collection_name=\"ai_usage_knowledge_index\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "print(\"âœ… Collection created successfully\")\n",
        "\n",
        "# Create the vector store\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"ai_usage_knowledge_index\",\n",
        "    embedding=embedding_model,\n",
        ")\n",
        "print(\"âœ… Vector store created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our vector database set-up, we can add our documents into it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = vector_store.add_documents(documents=ai_usage_knowledge_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating a Retriever\n",
        "\n",
        "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
        "\n",
        "This will involve a popular LangChain interace known as `as_retriever`!\n",
        "\n",
        "> NOTE: We can still specify how many documents we wish to retrieve per vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 4, '_id': 'f80322ff25ce44cebffb6ddb6b09f8a1', '_collection_name': 'ai_usage_knowledge_index'}, page_content='to Games and Role Play. In contrast, Zao-Sanders (2025) estimates that Therapy/Companionship is\\nthe most prevalent use case for generative AI.9\\nWe also document several important facts about demographic variation in ChatGPT usage. First,\\nwe show evidence that the gender gap in ChatGPT usage has likely narrowed considerably over time,\\nand may have closed completely. In the few months after ChatGPT was released about 80% of active\\nusers had typically masculine first names.10 However, that number declined to 48% as of June 2025,\\nwith active users slightly more likely to have typically feminine first names. Second, we find that\\nnearly half of all messages sent by adults were sent by users under the age of 26, although age gaps\\nhave narrowed somewhat in recent months. Third, we find that ChatGPT usage has grown relatively\\nfaster in low- and middle-income countries over the last year. Fourth, we find that educated users and\\nusers in highly-paid professional occupations are substantially more likely to use ChatGPT for work.\\nWe introduce a new taxonomy to classify messages according to the kind of output the user is\\nseeking, using a simple rubric that we call Asking, Doing, or Expressing.11\\nAsking is when the\\nuser is seeking information or clarification to inform a decision, corresponding to problem-solving\\nmodels of knowledge work (e.g., Garicano (2000); Garicano and Rossi-Hansberg (2006); Carnehl and\\nSchneider (2025); Ide and Talamas (2025)). Doing is when the user wants to produce some output\\nor perform a particular task, corresponding to classic task-based models of work (e.g., Autor et al.\\n(2003)). Expressing is when the user is expressing views or feelings but not seeking any information or\\naction. We estimate that about 49% of messages are Asking, 40% are Doing, and 11% are Expressing.\\nHowever, as of July 2025 about 56% of work-related messages are classified as Doing (e.g., performing\\njob tasks), and nearly three-quarters of those are Writing tasks. The relative frequency of writing-\\nrelated conversations is notable for two reasons. First, writing is a task that is common to nearly all\\nwhite-collar jobs, and good written communication skills are among the top â€œsoftâ€ skills demanded by\\nemployers (National Association of Colleges and Employers, 2024). Second, one distinctive feature of\\ngenerative AI, relative to other information technologies, is its ability to produce long-form outputs\\nsuch as writing and software code.\\nWe also map message content to work activities using the Occupational Information Network\\n(O*NET), a survey of job characteristics supported by the U.S. Department of Labor. We find that\\nabout 81% of work-related messages are associated with two broad work activities: 1) obtaining,\\ndocumenting, and interpreting information; and 2) making decisions, giving advice, solving problems,\\nand thinking creatively. Additionally, we find that the work activities associated with ChatGPT usage\\nare highly similar across very different kinds of occupations. For example, the work activities Getting\\nInformation and Making Decisions and Solving Problems are in the top five of message frequency in\\nnearly all occupations, ranging from management and business to STEM to administrative and sales\\noccupations.\\nOverall, we find that information-seeking and decision support are the most common ChatGPT'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 38, '_id': '74b7358427104bf88af04485a9228f22', '_collection_name': 'ai_usage_knowledge_index'}, page_content='References\\nAcemoglu, Daron, â€œThe Simple Macroeconomics of AI,â€ Technical Report 32487, National Bureau\\nof Economic Research, Cambridge, MA May 2024.\\nAutor, David H., Frank Levy, and Richard J. Murnane, â€œThe Skill Content of Recent Tech-\\nnological Change: An Empirical Exploration,â€ Quarterly Journal of Economics, November 2003,\\n118 (4), 1279â€“1333.\\nBengio, Yoshua, Aaron Courville, and Pascal Vincent, â€œRepresentation Learning: A Review\\nand New Perspectives,â€ 2014.\\nBick, Alexander, Adam Blandin, and David J. Deming, â€œThe Rapid Adoption of Generative\\nAI,â€ Technical Report 32966, National Bureau of Economic Research, Cambridge, MA September\\n2024.\\nCaplin, Andrew, David J. Deming, SÃ¸ren Leth-Petersen, and Ben Weidmann, â€œEconomic\\nDecision-Making Skill Predicts Income in Two Countries,â€ NBER Working Paper 31674, National\\nBureau of Economic Research, Cambridge, MA September 2023. Revised May 2024.\\nCarnehl, Christoph and Johannes Schneider, â€œA Quest for Knowledge,â€ Econometrica, March\\n2025, 93 (2), 623â€“659. Published March 2025.\\nChetty, Raj, Matthew O. Jackson, Theresa Kuchler, Johannes Stroebel, Nathaniel\\nHendren, Robert B. Fluegge, Sara Gong, Federico Gonzalez, Armelle Grondin,\\nMatthew Jacob, Drew Johnston, Martin Koenen, Eduardo Laguna-Muggenburg, Flo-\\nrian Mudekereza, Tom Rutter, Nicolaj Thor, Wilbur Townsend, Ruby Zhang, Mike\\nBailey, Pablo BarberÂ´a, Monica Bhole, and Nils Wernerfelt, â€œSocial Capital I: Measurement\\nand Associations with Economic Mobility,â€ Nature, 2022, 608 (7923), 108â€“121.\\nChiang, Wei-Lin, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li,\\nDacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and\\nIon Stoica, â€œChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,â€\\nin â€œProceedings of the 41st International Conference on Machine Learningâ€ ICMLâ€˜24 JMLR.org\\nVienna, Austria 2024, pp. 8359â€“8388.\\nCollis, Avinash and Erik Brynjolfsson, â€œAIâ€™s Overlooked $97 Billion Contribution to the Econ-\\nomy,â€ Wall Street Journal, August 2025.\\nDeming, David J., â€œThe Growing Importance of Decision-Making on the Job,â€ NBER Working\\nPaper 28733, National Bureau of Economic Research, Cambridge, MA April 2021.\\nEloundou, Tyna, Alex Beutel, David G. Robinson, Keren Gu, Anna-Luisa Brakman,\\nPamela Mishkin, Meghan Shah, Johannes Heidecke, Lilian Weng, and Adam Tau-\\nman Kalai, â€œFirst-Person Fairness in Chatbots,â€ in â€œThe Thirteenth International Conference on\\nLearning Representationsâ€ ICLR 2024 Singapore 2025.\\n37'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 17, '_id': '72517625cfb94bd18921d43da9707cd2', '_collection_name': 'ai_usage_knowledge_index'}, page_content='suggests that most user Writing conversations with ChatGPT are requests to modify user inputs\\nrather than to create something new. Education is a major use case for ChatGPT. 10.2% of all user\\nmessages and 36% of Practical Guidance messages are requests for Tutoring or Teaching. Another\\nlarge share - 8.5% in total and 30% of Practical Guidance - is general how-to advice on a variety\\nof topics. Technical Help includes Computer Programming (4.2% of messages), Mathematical Calcu-\\nlations (3%), and Data Analysis (0.4%). Looking at the topic of Self-Expression, only 2.4% of all\\nChatGPT messages are about Relationships and Personal Reflection (1.9%) or Games and Role Play\\n(0.4%).\\nWhile users can seek information and advice from traditional web search engines as well as from\\nChatGPT, the ability to produce writing, software code, spreadsheets, and other digital products\\ndistinguishes generative AI from existing technologies.\\nChatGPT is also more flexible than web\\nsearch even for traditional applications like Seeking Information and Practical Guidance, because\\nusers receive customized responses (e.g., tailored workout plans, new product ideas, ideas for fantasy\\nfootball team names) that represent newly generated content or novel modification of user-provided\\ncontent and follow-up requests.\\nFigure 9: Breakdown of granular conversation topic shares within the coarse mapping defined in Table 3. The\\nunderlying classifier prompt is available in Appendix A. Each bin reports a percentage of the total population.\\nShares are calculated from a sample of approximately 1.1 million sampled conversations from May 15, 2024\\nthrough June 26, 2025. Observations are reweighted to reflect total message volumes on a given day. Sampling\\ndetails available in Section 3.\\n5.3\\nUser Intent\\nExisting studies of the economic impacts of generative AI focus almost exclusively on the potential\\nfor AI to perform workplace tasks, either augmenting or automating human labor (e.g. Eloundou et\\nal. (2025), Handa et al. (2025), Tomlinson et al. (2025)). However, generative AI is a highly flexible\\n16'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 43, '_id': 'a642bb71688a442993ee345bda1cf15b', '_collection_name': 'ai_usage_knowledge_index'}, page_content='A.3\\nConversation Topic\\n-----\\nYou are an internal tool that classifies a message from a user to an AI chatbot,\\nbased on the context of the previous messages before it.\\n,â†’\\nBased on the last user message of this conversation transcript and taking into\\naccount the examples further below as guidance, please select the capability\\nthe user is clearly interested in, or `other` if it is clear but not in the\\nlist below, or `unclear` if it is hard to tell what the user even wants:\\n,â†’\\n,â†’\\n,â†’\\n- **edit_or_critique_provided_text**: Improving or modifying text provided by the\\nuser.\\n,â†’\\n- **argument_or_summary_generation**: Creating arguments or summaries on topics not\\nprovided in detail by the user.\\n,â†’\\n- **personal_writing_or_communication**: Assisting with personal messages, emails,\\nor social media posts.\\n,â†’\\n- **write_fiction**: Crafting poems, stories, or fictional content.\\n- **how_to_advice**: Providing step-by-step instructions or guidance on how to\\nperform tasks or learn new skills.\\n,â†’\\n- **creative_ideation**: Generating ideas or suggestions for creative projects or\\nactivities.\\n,â†’\\n- **tutoring_or_teaching**: Explaining concepts, teaching subjects, or helping the\\nuser understand educational material.\\n,â†’\\n- **translation**: Translating text from one language to another.\\n- **mathematical_calculation**: Solving math problems, performing calculations, or\\nworking with numerical data.\\n,â†’\\n- **computer_programming**: Writing code, debugging, explaining programming\\nconcepts, or discussing programming languages and tools.\\n,â†’\\n- **purchasable_products**: Inquiries about products or services available for\\npurchase.\\n,â†’\\n42'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 41, '_id': '28e5b1df823c42b18732f8f589ec79bd', '_collection_name': 'ai_usage_knowledge_index'}, page_content='Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\nGomez, Lukasz Kaiser, and Illia Polosukhin, â€œAttention Is All You Need,â€ in I. Guyon,\\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds., Ad-\\nvances in Neural Information Processing Systems, Vol. 30 of 31st Conference on Neural Information\\nProcessing Systems (NIPS) Curran Associates, Inc. Long Beach, CA, USA 2017.\\nWest, Jevin D., Jennifer Jacquet, Molly M. King, Shelley J. Correll, and Carl T.\\nBergstrom, â€œThe Role of Gender in Scholarly Authorship,â€ PLoS ONE, 2013, 8 (7), e66212.\\nWiggers, Kyle, â€œChatGPT Isnâ€™t the Only Chatbot Thatâ€™s Gaining Users,â€ TechCrunch, 2025. Ac-\\ncessed: 2025-09-10.\\nZao-Sanders, Marc, â€œHow People Are Really Using Gen AI in 2025,â€ Harvard Business Review\\nApril 2025. https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\\nZhao, Wenting, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng,\\nâ€œWildChat: 1M ChatGPT Interaction Logs in the Wild,â€ 2024.\\n40')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"How do people use AI in their daily work?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the Node\n",
        "\n",
        "We're finally ready to create our node!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "05qhncktIwK_"
      },
      "outputs": [],
      "source": [
        "def retrieve(state: State) -> State:\n",
        "  retrieved_docs = retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Node\n",
        "\n",
        "Next, let's create our `generate` node - which will leverage LangChain and something called an \"LCEL Chain\" which you can read more about [here](https://python.langchain.com/docs/concepts/lcel/)!\n",
        "\n",
        "We'll want to create a chain that does the following: \n",
        "\n",
        "1. Formats our inputs into a chat template suitable for RAG\n",
        "2. Takes that chat template and sends it to an LLM\n",
        "3. Parses that output into `str` format\n",
        "\n",
        "Let's get chaining!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chain Components: RAG Chat Template\n",
        "\n",
        "We'll create a chat template that takes in some query and formats it as a RAG prompt using LangChain's prompt template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context response with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#CONTEXT:\\nOUR CONTEXT HERE\\n\\nQUERY:\\nOUR QUERY HERE\\n\\nUse the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it\\'s not contained in the provided context response with \"I don\\'t know\"\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_prompt.invoke({\"context\" : \"OUR CONTEXT HERE\", \"query\" : \"OUR QUERY HERE\"}).messages[0].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Chain Components: Generator\n",
        "\n",
        "We'll next set-up the generator - which will be Ollama's `gpt-oss:20b` model running locally!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Using GPT-4 for fast and reliable responses\n",
        "ollama_chat_model = ChatOpenAI(model=\"gpt-4\", temperature=0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now call our model with a formatted prompt.\n",
        "\n",
        "Notice that we have some nested calls here - we'll see that this is made easier by LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 73, 'total_tokens': 80, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-CIypmsLdsKO6eofUO0vSthN17B47R', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--58d76761-f669-43fe-8580-8e16c282e0c0-0', usage_metadata={'input_tokens': 73, 'output_tokens': 7, 'total_tokens': 80, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ollama_chat_model.invoke(chat_prompt.invoke({\"context\" : \"Paris is the capital of France\", \"query\" : \"What is the capital of France?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chain Components: `str` Parser\n",
        "\n",
        "Finally, let's set-up our `StrOutputParser()` which will transform our model's output into a simple `str` to be provided to the user.\n",
        "\n",
        "> NOTE: You can see us leveraging LCEL in the example below to avoid needing to do nested calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The capital of France is Paris.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "generator_chain = chat_prompt | ollama_chat_model | StrOutputParser()\n",
        "\n",
        "generator_chain.invoke({\"context\" : \"Paris is the capital of France\", \"query\" : \"What is the capital of France?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `generate` Node: \n",
        "\n",
        "Now we can create our `generate` Node!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XiL2isC8JS0l"
      },
      "outputs": [],
      "source": [
        "def generate(state: State) -> State:\n",
        "  generator_chain = chat_prompt | ollama_chat_model | StrOutputParser()\n",
        "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
        "  return {\"response\" : response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtriMEcJxeR"
      },
      "source": [
        "Now we can start defining our graph!\n",
        "\n",
        "Think of the graph's state as a blank canvas that we can add nodes and edges to.\n",
        "\n",
        "Every graph starts with two special nodes - START and END - the act as the entry and exit point to the other nodes in the graphs.  \n",
        "\n",
        "All valid graphs must start at the START node and end at the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ia9IWM9AJ4bx"
      },
      "outputs": [],
      "source": [
        "# Start with the blank canvas\n",
        "graph_builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kro8bQEL2Yj"
      },
      "source": [
        "Now we can add a sequence to our \"canvas\" (graph) - this can be done by providing a list of nodes, the will automatically have edges that connect the i-th element to the i+1-th element in the list. The final element will be added to the END node unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OSfDMlXUL2kh"
      },
      "outputs": [],
      "source": [
        "graph_builder = graph_builder.add_sequence([retrieve, generate])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g79NZf5VL4en"
      },
      "source": [
        "Next, let's connect our START node to our `retrieve` node by adding an edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w1kTJKGNL4qA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1284e5970>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder.add_edge(START, \"retrieve\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EiVyt8-L6_5"
      },
      "source": [
        "Finally we can compile our graph! This will do basic verification to ensure that the Runnables have the correct inputs/outputs and can be matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TM4My6geL7FW"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvoQcfCP3xI"
      },
      "source": [
        "Finally, we can visualize our graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x122c60ce0>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRCjvvyP8DA"
      },
      "source": [
        "Let's take it for a spin!\n",
        "\n",
        "We invoke our graph like we do any other Runnable in LCEL!\n",
        "\n",
        "> NOTE: That's right, even a compiled graph is a Runnable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mSbsRLurKOKd",
        "outputId": "114185f3-4b98-4c66-96cd-65f4e4e3ef1d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "According to the documents, the most common ways people use AI, specifically ChatGPT, in their work include:\n",
              "\n",
              "1. Asking for Information: Users often use AI to seek information or clarification to inform a decision. This corresponds to problem-solving models of knowledge work.\n",
              "\n",
              "2. Doing Tasks: Users employ AI to produce some output or perform a particular task. This corresponds to classic task-based models of work. As of July 2025, about 56% of work-related messages are classified as Doing, with nearly three-quarters of those being Writing tasks. \n",
              "\n",
              "3. Writing Tasks: The relative frequency of writing-related conversations is notable. Writing is a common task in nearly all white-collar jobs, and AI's ability to produce long-form outputs such as writing and software code is a distinctive feature. \n",
              "\n",
              "4. Work Activities: About 81% of work-related messages are associated with two broad work activities: obtaining, documenting, and interpreting information; and making decisions, giving advice, solving problems, and thinking creatively.\n",
              "\n",
              "5. User Intent: AI is used in different ways based on user intent, classified into Asking, Doing, or Expressing. Doing, which includes drafting an email, writing code, etc., constitutes nearly 56% of work-related queries. \n",
              "\n",
              "6. Technical Help: AI is also used for technical help, including writing code, debugging, explaining programming concepts, or discussing programming languages and tools.\n",
              "\n",
              "Overall, the documents suggest that information-seeking and decision support are the most common AI usage in the workplace."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "response = graph.invoke({\"question\" : \"What are the most common ways people use AI in their work?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a_jEmE_rKwED",
        "outputId": "c5fac807-2a24-4cf9-8cca-105def13e3d8"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, people do use AI for their personal lives. The document reveals that non-work-related messages have grown from 53% to over 70% of all usage on ChatGPT, a generative AI model. This indicates that a significant portion of AI usage is for personal matters. Topics of conversation vary, but \"Practical Guidance,\" \"Seeking Information,\" and \"Writing\" are among the most common. However, areas like Computer Programming and Self-Expression represent relatively smaller shares of use."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = graph.invoke({\"question\" : \"Do people use AI for their personal lives?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The provided context does not contain specific information about the concerns or challenges that people have when using AI."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = graph.invoke({\"question\" : \"What concerns or challenges do people have when using AI?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't know"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = graph.invoke({\"question\" : \"Who is Batman?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_LRYXvvRjOp"
      },
      "source": [
        "#### â“ Question #2:\n",
        "LangGraph's graph-based approach lets us visualize and manage complex flows naturally. How could we extend our current implementation to handle edge cases? For example:\n",
        "- What if the retriever finds no relevant context?  \n",
        "- What if the response needs fact-checking?\n",
        "Consider how you would modify the graph to handle these scenarios.\n",
        "\n",
        "##### âœ… Answers\n",
        "\n",
        "**2.1 Handling No Relevant Context:**\n",
        "\n",
        "To handle cases where the retriever finds no relevant context, we could extend our graph with the following modifications:\n",
        "\n",
        "1. **Enhanced State**: Add a `context_quality` field to track retrieval success\n",
        "```python\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    context_quality: str  # \"good\", \"poor\", \"none\"\n",
        "    response: str\n",
        "```\n",
        "\n",
        "2. **Context Validation Node**: Add a new node to evaluate context quality\n",
        "```python\n",
        "def validate_context(state: State) -> State:\n",
        "    if not state[\"context\"] or len(state[\"context\"]) == 0:\n",
        "        return {\"context_quality\": \"none\"}\n",
        "    \n",
        "    # Check if retrieved docs have sufficient relevance scores\n",
        "    # or if they're too short/irrelevant\n",
        "    avg_relevance = calculate_relevance_score(state[\"context\"], state[\"question\"])\n",
        "    if avg_relevance < 0.3:  # threshold\n",
        "        return {\"context_quality\": \"poor\"}\n",
        "    \n",
        "    return {\"context_quality\": \"good\"}\n",
        "```\n",
        "\n",
        "3. **Conditional Routing**: Use LangGraph's conditional edges to route based on context quality\n",
        "```python\n",
        "def should_fallback(state: State) -> str:\n",
        "    if state[\"context_quality\"] in [\"none\", \"poor\"]:\n",
        "        return \"fallback_response\"\n",
        "    return \"generate\"\n",
        "\n",
        "# Add conditional edge\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"validate_context\",\n",
        "    should_fallback,\n",
        "    {\n",
        "        \"fallback_response\": \"generate_fallback\",\n",
        "        \"generate\": \"generate\"\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "4. **Fallback Response Node**: Handle cases with poor/no context\n",
        "```python\n",
        "def generate_fallback(state: State) -> State:\n",
        "    fallback_prompt = \"I don't have enough relevant information in my knowledge base to answer your question about: {question}. Could you please rephrase your question or provide more specific details?\"\n",
        "    return {\"response\": fallback_prompt.format(question=state[\"question\"])}\n",
        "```\n",
        "\n",
        "**2.2 Fact-Checking Implementation:**\n",
        "\n",
        "For fact-checking, we could add a verification layer to our graph:\n",
        "\n",
        "1. **Enhanced State**: Add fields for fact-checking\n",
        "```python\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "    needs_verification: bool\n",
        "    verified_response: str\n",
        "    confidence_score: float\n",
        "```\n",
        "\n",
        "2. **Fact-Checking Node**: Analyze response for factual claims\n",
        "```python\n",
        "def check_facts(state: State) -> State:\n",
        "    # Use a separate model or rule-based system to identify factual claims\n",
        "    factual_claims = extract_factual_claims(state[\"response\"])\n",
        "    \n",
        "    if len(factual_claims) > 0:\n",
        "        # Cross-reference with retrieved context\n",
        "        verification_results = []\n",
        "        for claim in factual_claims:\n",
        "            is_supported = verify_claim_against_context(claim, state[\"context\"])\n",
        "            verification_results.append(is_supported)\n",
        "        \n",
        "        confidence = sum(verification_results) / len(verification_results)\n",
        "        \n",
        "        if confidence < 0.7:  # Low confidence threshold\n",
        "            return {\n",
        "                \"needs_verification\": True,\n",
        "                \"confidence_score\": confidence\n",
        "            }\n",
        "    \n",
        "    return {\n",
        "        \"needs_verification\": False,\n",
        "        \"confidence_score\": 1.0,\n",
        "        \"verified_response\": state[\"response\"]\n",
        "    }\n",
        "```\n",
        "\n",
        "3. **Enhanced Response Generation**: Modify the generate node to include confidence indicators\n",
        "```python\n",
        "def generate_with_confidence(state: State) -> State:\n",
        "    if state[\"needs_verification\"]:\n",
        "        response = f\"{state['response']}\\n\\nâš ï¸ Note: Some information may need verification. Confidence: {state['confidence_score']:.2f}\"\n",
        "    else:\n",
        "        response = state[\"response\"]\n",
        "    \n",
        "    return {\"verified_response\": response}\n",
        "```\n",
        "\n",
        "4. **Updated Graph Flow**: \n",
        "```python\n",
        "# New flow: retrieve â†’ validate_context â†’ generate â†’ check_facts â†’ generate_with_confidence\n",
        "graph_builder = graph_builder.add_sequence([\n",
        "    retrieve, \n",
        "    validate_context, \n",
        "    generate, \n",
        "    check_facts, \n",
        "    generate_with_confidence\n",
        "])\n",
        "```\n",
        "\n",
        "**Additional Enhancements:**\n",
        "\n",
        "- **Logging Node**: Track all decisions and confidence scores for debugging\n",
        "- **User Feedback Loop**: Allow users to rate response quality and improve the system\n",
        "- **Multi-step Reasoning**: For complex questions, break them into sub-questions\n",
        "- **Source Attribution**: Always cite which documents were used for each claim\n",
        "\n",
        "This approach leverages LangGraph's strength in handling complex, conditional workflows while maintaining the modularity and debuggability that makes the system robust and maintainable."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
