{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama Setup and Testing\n",
        "\n",
        "This notebook will help you set up and test Ollama with LangChain connectors before starting the main RAG assignment.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Install Ollama** from https://ollama.ai\n",
        "   - On Linux/Mac: `curl https://ollama.ai/install.sh | sh`\n",
        "   - On Windows: Download and run the installer\n",
        "\n",
        "2. **Verify Installation**\n",
        "   - Run `ollama -v` in your terminal\n",
        "   - Should show version 0.11.10 or greater\n",
        "\n",
        "3. **Pull Required Models**\n",
        "   ```bash\n",
        "   # For chat/inference\n",
        "   ollama pull gpt-oss:20b\n",
        "   \n",
        "   # For embeddings\n",
        "   ollama pull embeddinggemma:latest\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Test Ollama Connection\n",
        "\n",
        "First, let's verify that Ollama is running and accessible:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ollama is running!\n",
            "\n",
            "Available models:\n",
            "  - embeddinggemma:latest\n",
            "  - gpt-oss:20b\n",
            "Requirement already satisfied: langchain-ollama in /Users/williamholt/.venv/lib/python3.12/site-packages (0.3.8)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-ollama) (0.5.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-ollama) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.29)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/williamholt/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /Users/williamholt/.venv/lib/python3.12/site-packages (from ollama<1.0.0,>=0.5.3->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/williamholt/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/williamholt/.venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/williamholt/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.3.1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Test if Ollama is running\n",
        "try:\n",
        "    response = requests.get('http://localhost:11434/api/tags')\n",
        "    if response.status_code == 200:\n",
        "        models = json.loads(response.text)\n",
        "        print(\"✅ Ollama is running!\")\n",
        "        print(\"\\nAvailable models:\")\n",
        "        for model in models.get('models', []):\n",
        "            print(f\"  - {model['name']}\")\n",
        "    else:\n",
        "        print(\"❌ Ollama is not responding properly\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"❌ Cannot connect to Ollama. Make sure it's running!\")\n",
        "    print(\"Start Ollama by running 'ollama serve' in a terminal\")\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain-ollama\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Embeddings with Ollama\n",
        "\n",
        "Now let's test creating embeddings using the LangChain Ollama connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = OllamaEmbeddings(\n",
        "    model=\"embeddinggemma:latest\",\n",
        "    base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
        ")\n",
        "\n",
        "print(\"✅ Embedding model initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding query: 'What is the meaning of life?'\n",
            "\n",
            "✅ Successfully created embedding!\n",
            "Embedding dimension: 768\n",
            "First 10 values: [-0.14651252, 0.02883863, 0.037894662, -0.024491956, -0.026222097, 0.015908629, -0.027572967, 0.027553586, 0.011392499, 3.1171552e-05]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding a single query\n",
        "test_query = \"What is the meaning of life?\"\n",
        "\n",
        "print(f\"Embedding query: '{test_query}'\")\n",
        "embedding = embedding_model.embed_query(test_query)\n",
        "\n",
        "print(f\"\\n✅ Successfully created embedding!\")\n",
        "print(f\"Embedding dimension: {len(embedding)}\")\n",
        "print(f\"First 10 values: {embedding[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding multiple documents...\n",
            "\n",
            "✅ Successfully created 3 embeddings!\n",
            "\n",
            "Document 1: 'The quick brown fox jumps over the lazy dog....'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.14795837, 0.0030012168, 0.05201346, -0.028874084, -0.036693174]\n",
            "\n",
            "Document 2: 'Machine learning is a subset of artificial intelli...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.12351282, -0.002831055, -0.00028308417, 0.009977267, 0.001662947]\n",
            "\n",
            "Document 3: 'Python is a popular programming language for data ...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.16275379, -0.010298255, 0.025541196, 0.0004042971, -0.01885671]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding multiple documents\n",
        "test_documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding multiple documents...\")\n",
        "embeddings = embedding_model.embed_documents(test_documents)\n",
        "\n",
        "print(f\"\\n✅ Successfully created {len(embeddings)} embeddings!\")\n",
        "for i, doc in enumerate(test_documents):\n",
        "    print(f\"\\nDocument {i+1}: '{doc[:50]}...'\")\n",
        "    print(f\"  Embedding dimension: {len(embeddings[i])}\")\n",
        "    print(f\"  First 5 values: {embeddings[i][:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Model Inference with Ollama\n",
        "\n",
        "Now let's test using Ollama for text generation/inference using the LangChain connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Chat model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Initialize the chat model\n",
        "chat_model = ChatOllama(\n",
        "    model=\"gpt-oss:20b\",\n",
        "    temperature=0.6,\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"✅ Chat model initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Let's add a procedure to measure the inference performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detailed_performance_metrics(response_metadata):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive performance metrics from Ollama response metadata\n",
        "    \"\"\"\n",
        "    # Extract all timing data (in nanoseconds)\n",
        "    total_duration = response_metadata.get('total_duration', 0)\n",
        "    load_duration = response_metadata.get('load_duration', 0)\n",
        "    prompt_eval_duration = response_metadata.get('prompt_eval_duration', 0)\n",
        "    eval_duration = response_metadata.get('eval_duration', 0)\n",
        "    \n",
        "    # Extract token counts\n",
        "    prompt_eval_count = response_metadata.get('prompt_eval_count', 0)\n",
        "    eval_count = response_metadata.get('eval_count', 0)\n",
        "    \n",
        "    # Convert to seconds\n",
        "    total_seconds = total_duration / 1_000_000_000\n",
        "    load_seconds = load_duration / 1_000_000_000\n",
        "    prompt_eval_seconds = prompt_eval_duration / 1_000_000_000\n",
        "    eval_seconds = eval_duration / 1_000_000_000\n",
        "    \n",
        "    # tokens per second\n",
        "    tokens_per_second = eval_count / eval_seconds\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'generation_tokens_per_second': eval_count / eval_seconds if eval_seconds > 0 else 0,\n",
        "        'prompt_tokens_per_second': prompt_eval_count / prompt_eval_seconds if prompt_eval_seconds > 0 else 0,\n",
        "        'total_tokens': prompt_eval_count + eval_count,\n",
        "        'total_time_seconds': total_seconds,\n",
        "        'load_time_seconds': load_seconds,\n",
        "        'generation_time_seconds': eval_seconds,\n",
        "        'prompt_processing_time_seconds': prompt_eval_seconds,\n",
        "        }\n",
        "\n",
        "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
        "    print(f\"Total time seconds: {metrics['total_time_seconds']}\")\n",
        "    print(f\"Load time seconds: {metrics['load_time_seconds']}\")\n",
        "    print(f\"Generation time seconds: {metrics['generation_time_seconds']}\")\n",
        "    print(f\"Prompt processing time seconds: {metrics['prompt_processing_time_seconds']}\")\n",
        "    print(f\"Generation tokens per second: {metrics['generation_tokens_per_second']}\")\n",
        "    print(f\"Prompt tokens per second: {metrics['prompt_tokens_per_second']}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Explain quantum computing in one sentence.\n",
            "\n",
            "Generating response...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: Quantum computing harnesses the principles of superposition, entanglement, and interference to process information with qubits, enabling certain problems to be solved exponentially faster than with classical bits.\n"
          ]
        }
      ],
      "source": [
        "# Test simple inference\n",
        "prompt = \"Explain quantum computing in one sentence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerating response...\")\n",
        "\n",
        "response = chat_model.invoke(prompt)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 240\n",
            "Total time seconds: 279.965131542\n",
            "Load time seconds: 0.364771667\n",
            "Generation time seconds: 270.741331708\n",
            "Prompt processing time seconds: 8.852558917\n",
            "Generation tokens per second: 0.6131313566080644\n",
            "Prompt tokens per second: 8.359164925510317\n"
          ]
        }
      ],
      "source": [
        "_ = detailed_performance_metrics(response.response_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending messages to model...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: **Machine learning is a way to let computers learn from data instead of being told every step by a programmer.**\n",
            "\n",
            "Think of it like this:\n",
            "\n",
            "| Step | What a human does | What a machine learning model does |\n",
            "|------|-------------------|-----------------------------------|\n",
            "| 1. | I show you many pictures of cats and dogs and tell you which is which. | The computer looks at all those pictures and records patterns (e.g., “cats have pointy ears, dogs have floppy ears”). |\n",
            "| 2. | You notice the patterns and can now guess if a new picture is a cat or a dog. | The computer uses the patterns it learned to guess the label of a brand‑new picture. |\n",
            "\n",
            "### How it works in short\n",
            "\n",
            "1. **Collect data** – Gather examples (images, text, numbers, etc.).\n",
            "2. **Choose a model** – Pick a mathematical “recipe” (e.g., a neural network, decision tree, etc.).\n",
            "3. **Train** – Show the model the data and let it adjust its internal settings so its predictions get better.\n",
            "4. **Test** – Check how well it works on new, unseen data.\n",
            "5. **Deploy** – Use the trained model to make predictions in the real world.\n",
            "\n",
            "### Why it matters\n",
            "\n",
            "- **Automation**: Tasks that would take humans hours (e.g., sorting emails, recognizing speech) can be done quickly.\n",
            "- **Personalization**: Recommending movies, ads, or products based on what you liked before.\n",
            "- **Insight**: Finding hidden patterns in huge data sets that humans might miss.\n",
            "\n",
            "### Everyday examples\n",
            "\n",
            "| Application | What it does | How ML helps |\n",
            "|-------------|--------------|--------------|\n",
            "| Spam filters | Blocks unwanted emails | Learns which words or patterns usually appear in spam |\n",
            "| Voice assistants (Siri, Alexa) | Understands spoken commands | Learns to map spoken words to actions |\n",
            "| Recommendation engines (Netflix, Amazon) | Suggests what to watch or buy | Learns your past preferences and similar users’ choices |\n",
            "| Self‑driving cars | Detects road signs, pedestrians | Learns from thousands of driving scenarios |\n",
            "\n",
            "### Key points to remember\n",
            "\n",
            "- **Data‑driven**: The model’s quality depends on the amount and quality of data.\n",
            "- **Not magic**: It’s still a mathematical algorithm; it’s not “thinking” like a human.\n",
            "- **Iterative**: You can keep improving it by feeding more data or tweaking the model.\n",
            "\n",
            "In short, machine learning is a set of techniques that let computers learn patterns from data and then use those patterns to make predictions or decisions automatically.\n"
          ]
        }
      ],
      "source": [
        "# Test with system message and human message\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant that explains complex topics simply.\"),\n",
        "    HumanMessage(content=\"What is machine learning?\")\n",
        "]\n",
        "\n",
        "print(\"Sending messages to model...\")\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 678\n",
            "Total time seconds: 958.906033291\n",
            "Load time seconds: 0.30346625\n",
            "Generation time seconds: 941.969781375\n",
            "Prompt processing time seconds: 16.620763209\n",
            "Generation tokens per second: 0.6221006359085232\n",
            "Prompt tokens per second: 5.535245213660393\n"
          ]
        }
      ],
      "source": [
        "_ = detailed_performance_metrics(response.response_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Streaming Response\n",
        "\n",
        "Ollama supports streaming responses, which is useful for real-time applications:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Write a haiku about artificial intelligence.\n",
            "\n",
            "Streaming response:\n",
            "----------------------------------------\n",
            "Silent circuits hum  \n",
            "Learning patterns in the dark  \n",
            "Mind of code, awake\n",
            "----------------------------------------\n",
            "\n",
            "✅ Streaming completed!\n"
          ]
        }
      ],
      "source": [
        "# Test streaming\n",
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nStreaming response:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for chunk in chat_model.stream(prompt):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n✅ Streaming completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "If all the tests above passed, you're ready to use Ollama with LangChain! Here's what we tested:\n",
        "\n",
        "✅ **Embeddings**: \n",
        "- Created embeddings for single queries\n",
        "- Created embeddings for multiple documents\n",
        "- Verified embedding dimensions\n",
        "\n",
        "✅ **Model Inference**:\n",
        "- Simple text generation\n",
        "- Chat with system and human messages\n",
        "- Streaming responses\n",
        "- Integration with LangChain chains\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Model Not Found**: Pull the required models (`ollama pull <model-name>`)\n",
        "2. **Slow Performance**: Ollama models run on CPU by default. For better performance:\n",
        "   - Use smaller models for testing\n",
        "   - Consider GPU acceleration if available\n",
        "3. **Memory Issues**: Large models require significant RAM. Try smaller variants if needed.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now you're ready to proceed with the main RAG assignment using Ollama!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
